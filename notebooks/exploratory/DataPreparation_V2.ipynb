{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5744ed4b-2219-466c-94eb-271be334a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7997d420-b3d7-4388-a069-6da9e193ca75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '/Users/beantown/PycharmProjects/jigsaw-puzzle-solver/data/hisfrag20/raw/hisfrag20/'\n",
    "test_path = '/Users/beantown/PycharmProjects/jigsaw-puzzle-solver/data/hisfrag20/raw/hisfrag20_test/'\n",
    "result_path = '/Users/beantown/PycharmProjects/jigsaw-puzzle-solver/data/hisfrag20/prepared/paris_as_csv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db66ebb5-f060-4dfd-87ef-c20eb832622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(data_path):\n",
    "    '''Helper Method which provids the path onto you local machine and the fragment ID, such that pairs can be created'''\n",
    "    # load file names train dataset\n",
    "    file_names = [splitext(f)[0] for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "\n",
    "    # load file name test dataset\n",
    "    #file_names_test = [splitext(f)[0] for f in listdir(data_path_test) if isfile(join(data_path_test, f))]\n",
    "\n",
    "    # Split the image naming in wirter, page and fragment\n",
    "    # For training\n",
    "    #file_names_parts = [i.split(\"_\") for i in file_names]\n",
    "    # For test\n",
    "    file_names_parts = [i.split(\"_\") for i in file_names]\n",
    "    df = pd.DataFrame.from_records(file_names_parts,columns=['writer_id', 'ID_of_original_papyrus','fragment_id'])\n",
    "    df['path_to_fragment_image'] = file_names\n",
    "    df['path_to_fragment_image'] = data_path +  df['path_to_fragment_image'].astype(str)\n",
    "    return df[['path_to_fragment_image', 'ID_of_original_papyrus']]\n",
    "\n",
    "df = get_info(data_path=train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "244d9d7e-123f-47f4-b18b-1cd97bb24e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GroupShuffleSplit(n_splits=2, test_size=.2, random_state=0)\n",
    "train_idx, val_idx = next(gs.split(df, groups=df.ID_of_original_papyrus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219d6628-1a4f-474d-bcb3-512127f40a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df.loc[train_idx]\n",
    "val = df.loc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d15a11-64f7-43fa-9ef6-3d735d79961c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ID_of_original_papyrus.isin(val.ID_of_original_papyrus).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c53f2886-5228-4d50-89b1-ec219f322f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81815, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bb6bc53-eb2c-42cd-b3bd-eb5df1a9f6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19892, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fad8106-3032-46bb-afd5-601abac82d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = get_info(data_path=test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b06779a4-1cc4-4352-bd02-de4c9ec8921a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ID_of_original_papyrus.isin(test.ID_of_original_papyrus).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d24c8a93-e6c0-4931-be68-ec4050f75b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.ID_of_original_papyrus.isin(test.ID_of_original_papyrus).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b24fe6a-0686-46af-85c7-7784ad98cd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pairs(K, data, IDList):\n",
    "    \"\"\"\n",
    "    used from: https://github.com/plnicolas/master-thesis/blob/master/Papy-S-Net/PairGenerator.py\n",
    "    Function to create fragment pairs given a Pandas DataFrame and a list of IDs.\n",
    "    Parameters:\n",
    "    ----------\n",
    "        - K: The number of pairs of each type (positive and negative) to sample. Duplicates will be dropped,\n",
    "        so the final number of pairs WILL be smaller than 2K\n",
    "        - Data: Pandas DataFrame with rows of the form [path_to_fragment_image, ID_of_original_papyrus]\n",
    "        - IDList: List containing the IDs of the papyri to sample fragments from\n",
    "    Returns:\n",
    "    --------\n",
    "        - pairs: A list of fragment pairs, of the form [path_to_frag1, path_to_frag2]\n",
    "        - labels: A list of labels, i.e. original papyrus IDs\n",
    "    \"\"\"\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "\n",
    "    # For each papyrus used for training\n",
    "    for index in IDList:\n",
    "        isIndex = data.iloc[:, 1] == index\n",
    "        isNotIndex = data.iloc[:, 1] != index\n",
    "        # List of images from the indexed papyrus\n",
    "        indexTrueList = data[isIndex].iloc[:, 0]\n",
    "        # List of images NOT from the indexed papyrus\n",
    "        indexFalseList = data[isNotIndex].iloc[:, 0]\n",
    "\n",
    "        # K negative pairs\n",
    "        p1List = indexTrueList.sample(n=K, replace=True, random_state=356)\n",
    "        p2List = indexFalseList.sample(n=K, replace=True, random_state=323)\n",
    "        for k in range(K):\n",
    "            pair = [p1List.values[k], p2List.values[k]]\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "                labels.append(1)\n",
    "\n",
    "        # K positive pairs\n",
    "        p1List = indexTrueList.sample(n=K, replace=True, random_state=362)\n",
    "        p2List = indexTrueList.sample(n=K, replace=True, random_state=316)\n",
    "        for k in range(K):\n",
    "            pair = [p1List.values[k], p2List.values[k]]\n",
    "            if pair not in pairs:\n",
    "                pairs.append(pair)\n",
    "                labels.append(0)\n",
    "\n",
    "    # Shuffle the pairs and label lists before returning them\n",
    "    # The two lists are shuffled at once with the same order, of course\n",
    "    tmp = list(zip(pairs, labels))\n",
    "    random.shuffle(tmp)\n",
    "    pairs, labels = zip(*tmp)\n",
    "\n",
    "    return pairs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fddd575d-43c2-40a5-9476-9384dfd4a881",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = sample_pairs(K=2, data=train, IDList=train.ID_of_original_papyrus.unique())\n",
    "X_val, y_val = sample_pairs(K=2, data=val, IDList=val.ID_of_original_papyrus.unique())\n",
    "X_test, y_test = sample_pairs(K=2, data=test, IDList=test.ID_of_original_papyrus.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319a77c6-b15e-49a8-8148-b0eaea062b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(X_train)\n",
    "train['y'] = y_train\n",
    "#train.to_csv(result_path + 'train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85bbfb49-9283-4e91-83aa-740b28a8c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.DataFrame(X_val)\n",
    "val['y'] = y_val\n",
    "#val.to_csv(result_path + 'val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59ace0d0-a519-4da4-91f9-638b882d5c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_test)\n",
    "test['y'] = y_test\n",
    "test.to_csv(result_path + 'test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d27414-efe7-4db8-b7d7-0e63fbc548b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PuzzleSolver",
   "language": "python",
   "name": "puzzlesolver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
